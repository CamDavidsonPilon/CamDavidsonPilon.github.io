---
layout: post
title:  "Exploring Human Psychology with Mechanical Turk Data"
date:   2014-10-23T00:25:36-04:00
redirect_from:
  - /blogs/napkin-folding/17543647-exploring-human-psychology-with-mechanical-turk-data
---

<p>This blog post is a little different: it's a whole data collection and data analysis story. I become interested in some theories from behavioural economics, and wanted to verify them. So I used Mechanical Turkers to gather data, and then did some exploratory data analysis in Python and Pandas (bonus: I recorded my data analysis and visualization, see below).</p>
<h3>Prospect Theory and Expected Values</h3>
<p>It's clear that humans are irrational, but how irrational are they? After some research into behavourial economics, I became very interested in Prospect Theory, a modern theory about human behaviours. Part of Prospect Theory is determining how our brains value different outcomes under uncertainty, that is, how expected values are mentally calculated. A very interesting part of Prospect theory is that it is <em>not</em> probabilities that are used in the calculation of expected value:</p>
<div class="figure"><img src="http://latex.codecogs.com/gif.latex?E%5E%7B%5Cmathbf%7BQ%7D%7D%5B&amp;space;Z&amp;space;%5D&amp;space;=&amp;space;%5Csum_%7Bz_i%7D&amp;space;q_i&amp;space;z_i"/></div>
<p>Here, the <em>q</em>'s are not the probabilities of outcome <em>z</em>, but it is from another probability measure called <em>decision weights</em> that humans actually use to weigh outcomes. Using a <a>change of measure</a>, we can observe the relationship between the actual probabilities and the decision weights:</p>
<div class="figure"><img src="http://latex.codecogs.com/gif.latex?E%5E%7B%5Cmathbf%7BQ%7D%7D%5B%20Z%20%5D%20%3D%20E%5E%7B%5Cmathbf%7BP%7D%7D%5Cleft%5B%20Z%20%5Cfrac%7BdQ%7D%7BdP%7D%5Cright%5D%20%3D%20%5Csum_%7Bz%7D%20p_i%20%5Cleft%28%5Cfrac%7Bq_i%7D%7Bp_i%7D%20z%5Cright%29"/></div>
<p>My interest is in this change of measure.</p>
<h3 id="the-setup">The Setup</h3>
<p>Suppose you have two choices:</p>
<ol style="list-style-type: decimal;">
<li>Lottery A: have a 1% chance to win $10 000,</li>
<li>Lottery B: have a 99% chance to win $101</li>
</ol>
<p>Which would you prefer?</p>
<p>Well, under the real world probabilty measure, these two choices are equal: .99 * 101 = .01 * 10000. Thus a rational human would be indifferent to either option. But an <em>actual</em>human would have a preference: they would see one more valuable than the other. Thus:</p>
<div class="figure"><img src="http://latex.codecogs.com/gif.latex?E%5E%7B%5Cmathbf%7BP%7D%7D%5B%20Z_1%20%5D%20%3D%20E%5E%7B%5Cmathbf%7BP%7D%7D%5B%20Z_2%20%5D%2C%20%5C%3B%5C%3B%20E%5E%7B%5Cmathbf%7BQ%7D%7D%5B%20Z_1%20%5D%20%3E%20E%5E%7B%5Cmathbf%7BQ%7D%7D%5B%20Z_2%20%5D"/></div>
<p>rewritten:</p>
<div class="figure"><img src="http://latex.codecogs.com/gif.latex?p_1Z_1%20%3D%20p_2Z_2%2C%20%5C%3B%5C%3B%20q_1Z_1%20%3E%20q_2Z_2"/></div>
<p>and dividing:</p>
<div class="figure"><img src="http://latex.codecogs.com/gif.latex?%5Cfrac%7Bq_1%7D%7Bp_1%7D%20%3E%20%5Cfrac%7Bq_2%7D%7Bp_2%7D%20%5C%5C%20%5CRightarrow%20%5Cfrac%7BdQ%7D%7BdP%7D%28p_1%29%20%3E%20%5Cfrac%7BdQ%7D%7BdP%7D%28p_2%29"/></div>
<p>What's left to do is determine the direction of the first inequality.</p>
<h3 id="mechanical-turk-it.">Mechanical Turk it.</h3>
<p>So I created combinations of probabilities and prizes, all with equal real-world expected value, and asked <a href="https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk">Mechanical Turkers</a> to pick which one they preferred:</p>
<div class="figure"><img alt="Imgur" src="//i.imgur.com/KHePN5a.png"/></div>
<p>The original HIT data and the python scripts that generate are below, plus the data that I just now recieved back from MTurk. Each pair of lotteries received 10 turkers votes.</p>
<p><strong>Note:</strong> The Turking cost me $88.40, if you'd like to give back over <a href="https://www.gittip.com/CamDavidsonPilon/">Gittip</a>, that would be great =)</p>
<p><strong>Note:</strong> I called the first choice <em>Lottery A</em> and the second choice <em>Lottery B</em>.</p>
<h3 id="analysis">Analysis</h3>
<p>Below is a slightly inappropriate heatmap of the choices people made. If everyone was rational, and hence indifferent to the two choices, the probabilities should hover around 0.5. This is clearly not the case.</p>
<div class="figure"><img alt="Imgur" src="//i.imgur.com/poAzHxZl.png"/></div>
<p>What else do we see here?</p>
<ol style="list-style-type: decimal;">
<li>As expected, people are loss averse: every point in the lower-diagonal is where lottery <em>A</em> had a high probability of success than <em>B</em>. The matrix shows that most points in here are greater than 50%, thus people chose the safer bet more often.</li>
<li>The exception to the above point is the fact that 1% is choosen more favourably over 2%. This is an instance of the <em>possibility effect</em>. People are indifferent between 1% and 2%, as they are both so rare, thus will pick the one with larger payoff.</li>
</ol>
<h3>Bonus: video of data munging in Pandas</h3>
<p>I recorded my steps to take the <a href="https://raw.githubusercontent.com/CamDavidsonPilon/decision-weights/master/cleaned_turk_results.csv">raw Turker data</a>, and create the visualization above:</p>
<iframe allowfullscreen="" frameborder="0" height="480" src="//www.youtube.com/embed/y2G1m9iKiKI?rel=0" width="640"></iframe>
<h3 id="faq">FAQ</h3>
<ol style="list-style-type: decimal;">
<li>
<p><strong>Why did I ask the Turkers to deeply imagine winning $50 dollars before answering the question?</strong> This was to offset a potential anchoring effect: if a Turkers first choice had prize $10 000, then any other prize would have looked pitiful, as the anchor had been set at $10 000. By having them imagine winning $50 (lower than any prize), then any prize they latter saw would appear better than this anchor.</p>
</li>
<li>
<p><strong>Next steps?</strong> I'd like to try this again, with more control over the Turkers (have a more diverse set of Turkers on it).</p>
</li>
<li>
<p><strong>Can I see the code and raw data?</strong>Sure, <a href="https://github.com/CamDavidsonPilon/decision-weights">here it is on Github</a>!</p>
</li>
</ol>
